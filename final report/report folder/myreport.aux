\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{Glossary of Abbreviations}{i}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Preface}{ii}{section*.3}\protected@file@percent }
\citation{rajabov2025}
\citation{bach2018}
\citation{evolutionverlan}
\citation{evolutionverlan}
\citation{rajabov2025}
\citation{rua2005}
\citation{hajiyeva2025}
\citation{deepl2020}
\citation{wu2016}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Context and Motivation}{1}{subsection.1.1}\protected@file@percent }
\citation{pei2019slang}
\citation{michel2018mtnt}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Google Translate cannot translate the verlan \textit  {tebie} correctly.}}{2}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:google_verlan}{{1}{2}{Google Translate cannot translate the verlan \textit {tebie} correctly}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces DeepL cannot translate the verlan \textit  {tebie} correctly.}}{2}{figure.caption.5}\protected@file@percent }
\newlabel{fig:deepl_verlan}{{2}{2}{DeepL cannot translate the verlan \textit {tebie} correctly}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces No desired translation for verlan \textit  {tebie} in DeepL's alternative word list.}}{2}{figure.caption.6}\protected@file@percent }
\newlabel{fig:deepl_alt_text}{{3}{2}{No desired translation for verlan \textit {tebie} in DeepL's alternative word list}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objective}{3}{subsection.1.2}\protected@file@percent }
\citation{mela1991verlan}
\citation{kaye1984syllabicite}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}A Living Verlan}{5}{subsection.2.1}\protected@file@percent }
\newlabel{eq:verlan-perm}{{1}{5}{A Living Verlan}{equation.1}{}}
\citation{mela1991verlan}
\citation{mela1991verlan}
\citation{mela1991verlan}
\citation{zurbuchen2024}
\citation{podhorna2020rapcor}
\citation{mekki2021tremolo}
\citation{panckhurst202088milsms}
\citation{pei2019slang}
\citation{sun2024informal}
\citation{slangornot2024}
\citation{wu2018slangsd}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Detecting Slang}{6}{subsection.2.2}\protected@file@percent }
\citation{russell1918soundex}
\citation{levenshtein1966}
\citation{wagner1974string}
\citation{wagner1974string}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}1910s-2010s: A Super-Condensed History of Slang Detection}{7}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How these methods work (high level).}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Performance and limitations for verlan/slang.}{7}{section*.10}\protected@file@percent }
\citation{damerau1964}
\citation{navarro2001approximate}
\citation{zobel1996phonetic}
\citation{philips1990metaphone}
\citation{philips2000doublemetaphone}
\citation{kukich1992techniques}
\citation{sproat2001normalization}
\citation{aw2006phrase}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Classical fuzzy-match methods: strengths and limitations for slang/verlan.}}{8}{table.caption.11}\protected@file@percent }
\newlabel{tab:fuzzy_perf}{{1}{8}{Classical fuzzy-match methods: strengths and limitations for slang/verlan}{table.caption.11}{}}
\citation{dhuliawala2016slangnet}
\citation{wu2018slangsd}
\citation{gupta2019slangzy}
\citation{dictionnaire2024chilleur}
\citation{beaufort2010hybrid}
\citation{han2011lexical}
\citation{baldwin2015shared}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}2016-2019: Dictionary Search}{9}{subsubsection.2.2.2}\protected@file@percent }
\citation{urban2020embeddings}
\citation{urban2020embeddings}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Meanwhile, for Fuzzy Search}{10}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}2020-2025: Fuzzy Search + Slang Corpus = BOOM}{10}{subsubsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Urban Dictionary embeddings (Wilson, 2020).}{10}{section*.12}\protected@file@percent }
\citation{slangornot2024}
\citation{slangornot2024}
\citation{slangornot2024}
\citation{sun2024informal}
\citation{sun2024informal}
\citation{slangornot2024}
\citation{urban2020embeddings}
\@writefile{toc}{\contentsline {paragraph}{Slang or Not? (2024).}{11}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Toward Informal Language Processing (NAACL 2024 Findings).}{11}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Detecting Verlan?}{11}{subsubsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Dataset}{13}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Separated Structures}{13}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Visualising the Dataset}{13}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Dataset at a glance (snapshot as of September 2025).}}{14}{table.caption.15}\protected@file@percent }
\newlabel{tab:dataset-glance}{{2}{14}{Dataset at a glance (snapshot as of September 2025)}{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Data Collection and Curation}{14}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Sampling}{14}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Balancing the Training Dataset}{16}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Evidence Levels (Source)}{17}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Evidence levels applied across the verlan dataset.}}{17}{table.caption.16}\protected@file@percent }
\newlabel{tab:verlan_tiers}{{3}{17}{Evidence levels applied across the verlan dataset}{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Dataset used in this report}{17}{subsection.3.4}\protected@file@percent }
\citation{jiang2023mistral7b}
\citation{jiang2023mistral7b}
\citation{touvron2023llama}
\citation{touvron2023llama2}
\@writefile{toc}{\contentsline {section}{\numberline {4}Building the Pipelines --- Model Architectures and Specifications}{18}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Mistral 7B\tmspace  +\thickmuskip {.2777em}---\tmspace  +\thickmuskip {.2777em}Why?}{18}{subsection.4.1}\protected@file@percent }
\citation{martin2019camembert}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Zero-Shot Models}{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Mistral 7B Prompt Engineering with Vibe}{19}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Zero-shot pipeline for Mistral}}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:mistral-zeroshot-pipeline}{{4}{19}{Zero-shot pipeline for Mistral}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Zero-shot reference model}{20}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Artificial Analysis Intelligence Index (retrieved 14 Oct 2025). Our zero-shot reference, \textit  {GPT-5 Codex (High)}, is the model we evaluate.}}{21}{figure.caption.19}\protected@file@percent }
\newlabel{fig:AI_Index}{{5}{21}{Artificial Analysis Intelligence Index (retrieved 14 Oct 2025). Our zero-shot reference, \textit {GPT-5 Codex (High)}, is the model we evaluate}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Training Models}{22}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}The Pipelines}{22}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A compact view of the four verlan identification pipelines.\footnotesize   Exp~A: Frozen Encoder + Logistic Regression classifier Exp~B: End-to-End Encoder + Linear classifier Exp~C: Frozen Encoder + BERT-inspired classifier Exp~D: End-to-End Encoder + BERT-inspired classifier}}{23}{figure.caption.20}\protected@file@percent }
\newlabel{fig:pipeline-overview}{{6}{23}{A compact view of the four verlan identification pipelines.\footnotesize \\Exp~A: Frozen Encoder + Logistic Regression classifier\\Exp~B: End-to-End Encoder + Linear classifier\\Exp~C: Frozen Encoder + BERT-inspired classifier\\Exp~D: End-to-End Encoder + BERT-inspired classifier}{figure.caption.20}{}}
\citation{alsharou2021noise}
\@writefile{toc}{\contentsline {paragraph}{Input}{24}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sanitise Text}{24}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why Not Preserve Upper Cases and Annotation Marks}{24}{section*.23}\protected@file@percent }
\citation{lodha2023surgical}
\@writefile{toc}{\contentsline {paragraph}{Tokenisation}{25}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Encoder}{25}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Post-encoder processing}{25}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Masking}{25}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Mean Pooling}{26}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\(\ell _{2}\) Normalisation}{26}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Classifiers\tmspace  +\thickmuskip {.2777em}---\tmspace  +\thickmuskip {.2777em}Logistic Regression and BERT-inspired MLPs}{27}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Logistic Regression}{27}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{BERT-Inspired Classifier}{27}{section*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Classic BERT classifier module.}}{28}{figure.caption.33}\protected@file@percent }
\newlabel{fig:bert-classifier-module}{{7}{28}{Classic BERT classifier module}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces BERT-style detection classifier (two-layer MLP on pooled Mistral embeddings).}}{29}{figure.caption.34}\protected@file@percent }
\newlabel{fig:bert-detect-classifier}{{8}{29}{BERT-style detection classifier (two-layer MLP on pooled Mistral embeddings)}{figure.caption.34}{}}
\citation{paszke2019pytorch}
\citation{loshchilov2019adamw}
\@writefile{toc}{\contentsline {subparagraph}{Loss and optimisation}{30}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Sigmoid Threshold}{30}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}The Usage of the Dataset}{30}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Environment and Hyperparameters}{31}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Environment}{31}{figure.caption.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hyperparameters}{31}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Seeds}{31}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Batch Size}{31}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Maximum Length}{32}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Quantisation}{32}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Epochs}{32}{section*.44}\protected@file@percent }
\citation{pearson1901pca}
\citation{maaten2008tsne}
\citation{mcinnes2018umap}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluations, Results, and Analyses}{33}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Evaluation Methodology}{33}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Embedding Space}{33}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces UMAP visualisations of the embedding space showing verlan tokens (orange) and standard French tokens (blue).}}{33}{figure.caption.45}\protected@file@percent }
\newlabel{fig:umap_comparison}{{9}{33}{UMAP visualisations of the embedding space showing verlan tokens (orange) and standard French tokens (blue)}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Testing Datasets}{34}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Testing Schema}{35}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Zero-shot Models}{35}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Number of Trials}{35}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Metrics}{35}{section*.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Binary confusion matrix (positive = verlan sentence, negative = standard sentence).}}{35}{figure.caption.49}\protected@file@percent }
\newlabel{fig:confusion-matrix-legend}{{10}{35}{Binary confusion matrix (positive = verlan sentence, negative = standard sentence)}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Results and Analyses}{35}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}General F1-Score and Accuracy}{36}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A general comparison of the F1 score and accuracy across models.}}{36}{figure.caption.50}\protected@file@percent }
\newlabel{fig:total-comparison}{{11}{36}{A general comparison of the F1 score and accuracy across models}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Common Verlan Performance by Model}{37}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Models' performance in common verlan identification.}}{37}{figure.caption.51}\protected@file@percent }
\newlabel{fig:historical-verlan-comparison}{{12}{37}{Models' performance in common verlan identification}{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Invented Verlan Performance by Model}{38}{subsubsection.5.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Invented verlan performance by model.}}{38}{figure.caption.52}\protected@file@percent }
\newlabel{fig:invented-verlan-comparison}{{13}{38}{Invented verlan performance by model}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Slang Controls Performance by Model}{39}{subsubsection.5.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Slang control accuracy.}}{39}{figure.caption.53}\protected@file@percent }
\newlabel{fig:slang-comparison}{{14}{39}{Slang control accuracy}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {paragraph}{A Discussion on the Hook}{40}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}And Yet Something Advanced}{40}{subsubsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Is E2E+BERT (Experiment~D) the Real King?}{40}{section*.55}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Recall--specificity trade-off across models.}}{41}{figure.caption.56}\protected@file@percent }
\newlabel{fig:tradeoff-scatter}{{15}{41}{Recall--specificity trade-off across models}{figure.caption.56}{}}
\@writefile{toc}{\contentsline {paragraph}{Small Dataset Caused More Instability?}{41}{section*.57}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Invented verlan recall stability.}}{42}{figure.caption.58}\protected@file@percent }
\newlabel{fig:invented-variance}{{16}{42}{Invented verlan recall stability}{figure.caption.58}{}}
\@writefile{toc}{\contentsline {paragraph}{Again: Is E2E+BERT (Experiment~D) the Real King?}{42}{section*.59}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Link between slang false alarms and overall false positives.}}{43}{figure.caption.60}\protected@file@percent }
\newlabel{fig:slang-fp-correlation}{{17}{43}{Link between slang false alarms and overall false positives}{figure.caption.60}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion and Limitation}{44}{section.6}\protected@file@percent }
\bibcite{rajabov2025}{1}
\bibcite{bach2018}{2}
\bibcite{evolutionverlan}{3}
\bibcite{rua2005}{4}
\bibcite{hajiyeva2025}{5}
\bibcite{deepl2020}{6}
\bibcite{wu2016}{7}
\bibcite{michel2018mtnt}{8}
\bibcite{zurbuchen2024}{9}
\bibcite{podhorna2020rapcor}{10}
\bibcite{mekki2021tremolo}{11}
\bibcite{panckhurst202088milsms}{12}
\bibcite{pei2019slang}{13}
\bibcite{sun2024informal}{14}
\bibcite{slangornot2024}{15}
\bibcite{dhuliawala2016slangnet}{16}
\bibcite{wu2018slangsd}{17}
\bibcite{gupta2019slangzy}{18}
\bibcite{dictionnaire2024chilleur}{19}
\bibcite{mela1991verlan}{20}
\bibcite{kaye1984syllabicite}{21}
\bibcite{russell1918soundex}{22}
\bibcite{levenshtein1966}{23}
\bibcite{philips1990metaphone}{24}
\bibcite{philips2000doublemetaphone}{25}
\bibcite{kukich1992techniques}{26}
\bibcite{sproat2001normalization}{27}
\bibcite{aw2006phrase}{28}
\bibcite{beaufort2010hybrid}{29}
\bibcite{han2011lexical}{30}
\bibcite{baldwin2015shared}{31}
\bibcite{urban2020embeddings}{32}
\bibcite{sun2024knowledge}{33}
\bibcite{jiang2023mistral7b}{34}
\bibcite{touvron2023llama}{35}
\bibcite{touvron2023llama2}{36}
\bibcite{martin2019camembert}{37}
\bibcite{du2025deepresearch}{38}
\bibcite{dong2024imbalance}{39}
\bibcite{alsharou2021noise}{40}
\bibcite{lodha2023surgical}{41}
\bibcite{paszke2019pytorch}{42}
\bibcite{loshchilov2019adamw}{43}
\bibcite{kingma2014adam}{44}
\bibcite{pearson1901pca}{45}
\bibcite{maaten2008tsne}{46}
\bibcite{mcinnes2018umap}{47}
\bibcite{wagner1974string}{48}
\bibcite{navarro2001approximate}{49}
\bibcite{damerau1964}{50}
\bibcite{zobel1996phonetic}{51}
\@writefile{toc}{\contentsline {section}{Appendix A: Expanded Evaluation Artefacts}{I}{section*.62}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Hold-out test aggregates (20 seeds) for trained detectors. Percentages report mean $\pm $ standard deviation across seeds; counts are means.}}{I}{table.caption.63}\protected@file@percent }
\newlabel{tab:appendix-holdout-aggregates}{{4}{I}{Hold-out test aggregates (20 seeds) for trained detectors. Percentages report mean $\pm $ standard deviation across seeds; counts are means}{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Targeted-slice comparison across 20 seeds. Historical and invented columns are verlan recalls; slang column measures rejection accuracy on contemporary slang controls.}}{I}{table.caption.64}\protected@file@percent }
\newlabel{tab:appendix-targeted-metrics}{{5}{I}{Targeted-slice comparison across 20 seeds. Historical and invented columns are verlan recalls; slang column measures rejection accuracy on contemporary slang controls}{table.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Zero-shot targeted breakdowns (single pass). Metrics computed on 29 historical verlan, 25 invented verlan, and 25 slang sentences, respectively.}}{I}{table.caption.65}\protected@file@percent }
\newlabel{tab:appendix-zeroshot-targeted}{{6}{I}{Zero-shot targeted breakdowns (single pass). Metrics computed on 29 historical verlan, 25 invented verlan, and 25 slang sentences, respectively}{table.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Zero-shot recall lift on invented verlan relative to the best trained detector (Frozen+BERT).}}{II}{figure.caption.66}\protected@file@percent }
\newlabel{fig:invented-lift}{{18}{II}{Zero-shot recall lift on invented verlan relative to the best trained detector (Frozen+BERT)}{figure.caption.66}{}}
\@writefile{toc}{\contentsline {section}{Appendix B: Dataset schema (detailed)}{III}{section*.67}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Detailed field-level schema for the dataset (columns, types, and relations between the lexicon and the sentence corpus).}}{III}{figure.caption.68}\protected@file@percent }
\newlabel{fig:dataset-schema-detailed}{{19}{III}{Detailed field-level schema for the dataset (columns, types, and relations between the lexicon and the sentence corpus)}{figure.caption.68}{}}
\@writefile{toc}{\contentsline {section}{Appendix C: Source inventory and crawl logs}{IV}{section*.69}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Source buckets referenced in \texttt  {Sentences\_balanced.xlsx}. Counts come from the current spreadsheet (URLs and textual citations combined).}}{V}{table.caption.70}\protected@file@percent }
\newlabel{tab:source-inventory}{{7}{V}{Source buckets referenced in \texttt {Sentences\_balanced.xlsx}. Counts come from the current spreadsheet (URLs and textual citations combined)}{table.caption.70}{}}
\@writefile{toc}{\contentsline {section}{Appendix D: Participant Information Sheet}{VI}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendix E: Participant Consent Form}{VIII}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendix F: Annotated Lexicon}{IX}{appendix.C}\protected@file@percent }
\gdef \@abspage@last{71}
