# -*- coding: utf-8 -*-
"""
Verlan Conversion SFT on Mistral-7B (A4000 16GB Optimized)
- é‡åŒ–ï¼š4-bit NF4  (BitsAndBytesConfig)
- LoRAï¼šB æª”ï¼ˆå« q/k/v/o + gate_proj + up_proj + down_projï¼‰
- è¨“ç·´åƒæ•¸ï¼šA æª”ï¼ˆå®‰å…¨ç©©å®šï¼‰
- å…¶ä»–ï¼špacking=Trueã€max_seq_length=768ã€TF32 é–‹å•Ÿã€å³å´ paddingã€pad_to_multiple_of=8

æ¸¬éç’°å¢ƒï¼ˆåƒè€ƒï¼‰ï¼štransformers>=4.41, peft>=0.11, trl>=0.9, bitsandbytes>=0.43, torch>=2.2
"""

import os
import random
import pandas as pd
import torch
from datasets import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    BitsAndBytesConfig,
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

from trl import SFTTrainer

# ------------------------------ å›ºå®šè¶…åƒ/æª”å ------------------------------
BASE_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
NEW_TOK_FILE = "GazetteerEntries.xlsx"   # ä½ çš„è‡ªå®šè©è¡¨ï¼ˆverlan_formï¼‰
CSV_FILE     = "verlan_pairs.csv"        # è¨“ç·´å°é½Šè³‡æ–™ï¼ˆsrc,tgtï¼‰
OUT_DIR      = "mistral-verlan-conv"
MAX_SEQ_LEN  = 768                       # 16GB é¡¯å­˜å»ºè­° 512~1024ï¼›å– 768 æŠ˜è¡·
SEED         = 42

# ------------------------------ åŸºæœ¬ç’°å¢ƒå„ªåŒ– ------------------------------
# å»ºè­°é…ç½®ï¼ˆæ¸›å°‘ç¢ç‰‡ï¼æé«˜ååï¼‰
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,garbage_collection_threshold:0.6"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
torch.backends.cuda.matmul.allow_tf32 = True         # Ampereï¼ˆA4000ï¼‰å•Ÿç”¨ TF32
torch.set_float32_matmul_precision("high")
os.makedirs("logs", exist_ok=True)                   # è®“ `tee logs/*.log` ä¸å†å ±ä¸å­˜åœ¨

# ------------------------------ Tokenizer ------------------------------
print("Loading tokenizer â€¦ GPU mode")
# NOTE: use_auth_token å·²æ£„ç”¨ï¼Œé€™è£¡ä»ä¿ç•™ï¼›è‹¥ä½ å·²åœ¨ CLI é…ç½® HF_TOKEN ä¹Ÿå¯çœç•¥
tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_auth_token=True)
tok.padding_side = "right"                            # é¿å… fp16 ä¸‹çš„æº¢ä½/é®ç½©ç•°å¸¸
if tok.pad_token is None:
    tok.pad_token = tok.eos_token

# è‡ªå®šç¾© Verlan è©å…ƒåŠ å…¥
added = 0
if os.path.exists(NEW_TOK_FILE):
    vlist = (
        pd.read_excel(NEW_TOK_FILE)["verlan_form"]
        .dropna()
        .astype(str)
        .str.lower()
        .unique()
        .tolist()
    )
    # åƒ…æ·»åŠ  tokenizer è£¡æ²’æœ‰çš„è©
    to_add = [v for v in vlist if v not in tok.get_vocab()]
    added = tok.add_tokens(to_add)
    print(f"Added {added} Verlan tokens to vocab.")

# ------------------------------ Dataset ------------------------------
print("Loading verlan_pairs.csv â€¦")
df = pd.read_csv(CSV_FILE)

PROMPT_TMPL = (
    "### Instruction: Convert any Verlan words back to standard French.\n"
    "### Input: {src}\n"
    "### Response: {tgt}"
)

# è½‰ç‚ºå–®åˆ— textï¼Œä¾› SFTTrainer ä½¿ç”¨ï¼ˆpacking æ¨¡å¼æœ€çœ paddingï¼‰
ds = Dataset.from_pandas(df)
ds = ds.map(
    lambda ex: {"text": PROMPT_TMPL.format(src=ex["src"], tgt=ex["tgt"])},
    remove_columns=list(df.columns),
)

# é˜²æ¥µç«¯è¶…é•·æ¨£æœ¬ï¼ˆé›™ä¿éšªï¼›SFT ä»æœƒè£åˆ° MAX_SEQ_LENï¼‰
def clip_len(x, hard_max=2048):
    t = x["text"]
    return {"text": t[:hard_max]}

ds = ds.map(clip_len)

# ------------------------------ æ¨¡å‹ï¼š4-bit é‡åŒ– ------------------------------
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

print("Loading base model with 4-bit quantization â€¦")
# å¦‚å·²å®‰è£ flash-attn2ï¼Œå¯æŠŠ attn_implementation è¨­ç‚º "flash_attention_2"
# è‹¥æœªå®‰è£ï¼Œè«‹ç§»é™¤è©²åƒæ•¸ï¼ˆæœƒè‡ªå‹•ç”¨ PyTorch SDPAï¼‰
try:
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_cfg,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        use_auth_token=True,
    )
except Exception:
    # å›é€€åˆ° PyTorch SDPA
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_cfg,
        device_map="auto",
        torch_dtype=torch.float16,
        use_auth_token=True,
    )

# æ–° token -> éœ€è¦æ“´å±• embedding / lm_headï¼ˆå°é½Šåˆ° 8ï¼Œåˆ©æ–¼ kernel å°é½Šï¼‰
if added > 0:
    model.resize_token_embeddings(len(tok), pad_to_multiple_of=8)

# QLoRA å¿…å‚™ï¼šå°é‡åŒ–æ¨¡å‹åšè¨“ç·´å‰æº–å‚™ï¼ˆnorm/cast/grad è¨­ç½®ï¼‰
model = prepare_model_for_kbit_training(model)
model.config.use_cache = False  # è¨“ç·´æœŸé—œé–‰ KV cache

# ------------------------------ LoRAï¼ˆB æª”ï¼šå« MLPï¼‰ ------------------------------
# Aï¼šq_proj,k_proj,v_proj,o_projï¼ˆè¼•é‡ï¼‰
# Bï¼šå†åŠ  gate_proj, up_proj, down_projï¼ˆæ•ˆæœâ†‘ï¼Œè¨ˆç®—â†‘ï¼‰
lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

# ------------------------------ TrainingArgumentsï¼ˆA æª”ï¼šå®‰å…¨ç©©å®šï¼‰ ------------------------------
args = TrainingArguments(
    output_dir=OUT_DIR,
    per_device_train_batch_size=2,     # å°æ‰¹æ¬¡ + æ¢¯åº¦ç´¯ç©ï¼Œç©©
    gradient_accumulation_steps=8,     # ç­‰æ•ˆ batch â†‘
    num_train_epochs=1,
    bf16=True,
    fp16=False,
    learning_rate=1e-4,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none",                  # å…ˆé—œæ‰ wandb
    gradient_checkpointing=True,       # çœé¡¯å­˜ï¼ˆå¿…é–‹ï¼‰
    gradient_checkpointing_kwargs={"use_reentrant": False},
    optim="paged_adamw_32bit",         # 4-bit é…é€™å€‹æ›´ç©©
    dataloader_num_workers=2,
    seed=SEED,
)

# ------------------------------ å•Ÿå‹• SFTï¼ˆpacking=Trueï¼‰ ------------------------------
print("Starting training â€¦ ğŸï¸")
trainer = SFTTrainer(
    model=model,
    train_dataset=ds,
    tokenizer=tok,
    args=args,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LEN,
    packing=True,                      # é—œéµï¼šæŠŠçŸ­æ¨£æœ¬æ‰“åŒ…ï¼Œpadding æµªè²»å¤§å¹…ä¸‹é™
)

trainer.train()
trainer.model.save_pretrained(OUT_DIR)
print("Convert model done.")
